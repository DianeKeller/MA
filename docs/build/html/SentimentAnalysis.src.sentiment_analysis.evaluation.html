<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>SentimentAnalysis.src.sentiment_analysis.evaluation package &mdash; SentimentAnalysis 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="_static/graphviz.css?v=4ae1632d" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js?v=2709fde1"></script>
        <script src="_static/doctools.js?v=9bcbadda"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="SentimentAnalysis.src.sentiment_analysis.prompt_engineering package" href="SentimentAnalysis.src.sentiment_analysis.prompt_engineering.html" />
    <link rel="prev" title="SentimentAnalysis.src.sentiment_analysis package" href="SentimentAnalysis.src.sentiment_analysis.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            SentimentAnalysis
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">SentimentAnalysis</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="SentimentAnalysis.html">SentimentAnalysis package</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="SentimentAnalysis.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="SentimentAnalysis.src.html">SentimentAnalysis.src package</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="SentimentAnalysis.html#submodules">Submodules</a></li>
<li class="toctree-l3"><a class="reference internal" href="SentimentAnalysis.html#sentimentanalysis-action-module">SentimentAnalysis.action module</a></li>
<li class="toctree-l3"><a class="reference internal" href="SentimentAnalysis.html#sentimentanalysis-constants-module">SentimentAnalysis.constants module</a></li>
<li class="toctree-l3"><a class="reference internal" href="SentimentAnalysis.html#sentimentanalysis-logger-module">SentimentAnalysis.logger module</a></li>
<li class="toctree-l3"><a class="reference internal" href="SentimentAnalysis.html#module-SentimentAnalysis.main">SentimentAnalysis.main module</a></li>
<li class="toctree-l3"><a class="reference internal" href="SentimentAnalysis.html#module-SentimentAnalysis.settings">SentimentAnalysis.settings module</a></li>
<li class="toctree-l3"><a class="reference internal" href="SentimentAnalysis.html#module-SentimentAnalysis.setup">SentimentAnalysis.setup module</a></li>
<li class="toctree-l3"><a class="reference internal" href="SentimentAnalysis.html#module-SentimentAnalysis.type_aliases">SentimentAnalysis.type_aliases module</a></li>
<li class="toctree-l3"><a class="reference internal" href="SentimentAnalysis.html#module-SentimentAnalysis.view_log">SentimentAnalysis.view_log module</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">SentimentAnalysis</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="modules.html">SentimentAnalysis</a></li>
          <li class="breadcrumb-item"><a href="SentimentAnalysis.html">SentimentAnalysis package</a></li>
          <li class="breadcrumb-item"><a href="SentimentAnalysis.src.html">SentimentAnalysis.src package</a></li>
          <li class="breadcrumb-item"><a href="SentimentAnalysis.src.sentiment_analysis.html">SentimentAnalysis.src.sentiment_analysis package</a></li>
      <li class="breadcrumb-item active">SentimentAnalysis.src.sentiment_analysis.evaluation package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/SentimentAnalysis.src.sentiment_analysis.evaluation.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="sentimentanalysis-src-sentiment-analysis-evaluation-package">
<h1>SentimentAnalysis.src.sentiment_analysis.evaluation package<a class="headerlink" href="#sentimentanalysis-src-sentiment-analysis-evaluation-package" title="Link to this heading"></a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Link to this heading"></a></h2>
</section>
<section id="module-SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation">
<span id="sentimentanalysis-src-sentiment-analysis-evaluation-all-languages-evaluation-module"></span><h2>SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation module<a class="headerlink" href="#module-SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation" title="Link to this heading"></a></h2>
<section id="all-languages-evaluation-py">
<h3>all_languages_evaluation.py<a class="headerlink" href="#all-languages-evaluation-py" title="Link to this heading"></a></h3>
<p>Version 1.0, updated on 2024-12-14</p>
</section>
<dl class="py class">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.</span></span><span class="sig-name descname"><span class="pre">AllLanguagesEvaluation</span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/all_languages_evaluation.html#AllLanguagesEvaluation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">MetricsVisualizationMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LoggingMixin</span></code></p>
<p>AllLanguagesEvaluation class</p>
<div class="graphviz"><img src="_images/inheritance-d949ecef8c36ae0c485a7d6a47e002adde3757cc.png" alt="Inheritance diagram of SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation" usemap="#inheritance5ec0997c27" class="inheritance graphviz" /></div>
<map id="inheritance5ec0997c27" name="inheritance5ec0997c27">
<area shape="rect" id="node1" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation" target="_top" title="AllLanguagesEvaluation class" alt="" coords="385,5,559,31"/>
</map><dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/all_languages_evaluation.html#AllLanguagesEvaluation.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation.__init__" title="Link to this definition"></a></dt>
<dd><p>Constructor.</p>
<p>Initializes the AllLanguagesEvaluation class.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation._compute_mean_metrics">
<span class="sig-name descname"><span class="pre">_compute_mean_metrics</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/all_languages_evaluation.html#AllLanguagesEvaluation._compute_mean_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation._compute_mean_metrics" title="Link to this definition"></a></dt>
<dd><p>Computes the mean metrics DataFrame.</p>
<p>Computes the mean metrics from the aggregated metrics and stores the
resulting DataFrame in the mean_metrics property of this class.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation.add_to_overall_metrics">
<span class="sig-name descname"><span class="pre">add_to_overall_metrics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">metrics</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="SentimentAnalysis.src.data_structures.html#SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame" title="SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame"><span class="pre">MyDataFrame</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/all_languages_evaluation.html#AllLanguagesEvaluation.add_to_overall_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation.add_to_overall_metrics" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation.add_to_overall_sentiment_data">
<span class="sig-name descname"><span class="pre">add_to_overall_sentiment_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Chunk</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/all_languages_evaluation.html#AllLanguagesEvaluation.add_to_overall_sentiment_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation.add_to_overall_sentiment_data" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation.get_mean_data_frame">
<span class="sig-name descname"><span class="pre">get_mean_data_frame</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">df</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">divisor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">DataFrame</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/all_languages_evaluation.html#AllLanguagesEvaluation.get_mean_data_frame"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation.get_mean_data_frame" title="Link to this definition"></a></dt>
<dd><p>Computes the means of all values in the DataFrame.</p>
<p>Uses the provided divisor to calculate the means.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The mean data.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation.lang_pred_freq_dict">
<span class="sig-name descname"><span class="pre">lang_pred_freq_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">language</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/all_languages_evaluation.html#AllLanguagesEvaluation.lang_pred_freq_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation.lang_pred_freq_dict" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation.show_freqs_comparison_by_prompt">
<span class="sig-name descname"><span class="pre">show_freqs_comparison_by_prompt</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">freqs_by_language</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/all_languages_evaluation.html#AllLanguagesEvaluation.show_freqs_comparison_by_prompt"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation.show_freqs_comparison_by_prompt" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation.show_freqs_comparisons_by_prompt">
<span class="sig-name descname"><span class="pre">show_freqs_comparisons_by_prompt</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/all_languages_evaluation.html#AllLanguagesEvaluation.show_freqs_comparisons_by_prompt"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation.show_freqs_comparisons_by_prompt" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation.show_mean_metrics">
<span class="sig-name descname"><span class="pre">show_mean_metrics</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/all_languages_evaluation.html#AllLanguagesEvaluation.show_mean_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation.show_mean_metrics" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation.show_overall_rankings">
<span class="sig-name descname"><span class="pre">show_overall_rankings</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/all_languages_evaluation.html#AllLanguagesEvaluation.show_overall_rankings"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation.show_overall_rankings" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation">
<span id="sentimentanalysis-src-sentiment-analysis-evaluation-classification-evaluation-module"></span><h2>SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation module<a class="headerlink" href="#module-SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation" title="Link to this heading"></a></h2>
<section id="classification-evaluation-py">
<h3>classification_evaluation.py<a class="headerlink" href="#classification-evaluation-py" title="Link to this heading"></a></h3>
<p>Version 1.0, updated on 2025-05-01</p>
</section>
<dl class="py class">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.</span></span><span class="sig-name descname"><span class="pre">ClassificationEvaluation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">correct_labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Series</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predicted_labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Series</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/classification_evaluation.html#ClassificationEvaluation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>ClassificationEvaluation class.</p>
<p>This class provides the tools for the evaluation of classification results.</p>
<div class="graphviz"><img src="_images/inheritance-6ef8b692dd451f03b77a6456b7cbeaf9bcb3efcb.png" alt="Inheritance diagram of SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation" usemap="#inheritanced365ae29f3" class="inheritance graphviz" /></div>
<map id="inheritanced365ae29f3" name="inheritanced365ae29f3">
<area shape="rect" id="node1" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation" target="_top" title="ClassificationEvaluation class." alt="" coords="5,5,176,31"/>
</map><dl class="py attribute">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation.LABELS">
<span class="sig-name descname"><span class="pre">LABELS</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['positive',</span> <span class="pre">'negative',</span> <span class="pre">'neutral']</span></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation.LABELS" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">correct_labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Series</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predicted_labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Series</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/classification_evaluation.html#ClassificationEvaluation.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation.__init__" title="Link to this definition"></a></dt>
<dd><p>Initiates the ClassificationEvaluation class with the given parameters.</p>
<p>Initiates the ClassificationEvaluation class with correct and
predicted labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>str</em>) – The name of the performed classification, used to name the set
of metrics.</p></li>
<li><p><strong>correct_labels</strong> (<em>Series</em>) – The correct labels to compare the classification results to.</p></li>
<li><p><strong>predicted_labels</strong> (<em>Series</em>) – The labels predicted in the classification process, to be
compared to the correct labels.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation._compute_average_metrics">
<span class="sig-name descname"><span class="pre">_compute_average_metrics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">metric_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">func</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/classification_evaluation.html#ClassificationEvaluation._compute_average_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation._compute_average_metrics" title="Link to this definition"></a></dt>
<dd><p>Computes average metrics and sets them in the metrics dictionary.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation._compute_metrics_per_label">
<span class="sig-name descname"><span class="pre">_compute_metrics_per_label</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">metric_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">func</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/classification_evaluation.html#ClassificationEvaluation._compute_metrics_per_label"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation._compute_metrics_per_label" title="Link to this definition"></a></dt>
<dd><p>Computes metrics for each class.</p>
<p>Computes metrics for each class and sets them in the metrics
dictionary.</p>
<p class="rubric">Notes</p>
<p>The zero_division=0 parameter makes the precision, recall and f1
metrics ignore samples for which no predicted label is given,
to prevent division by zero errors.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation._get_metric">
<span class="sig-name descname"><span class="pre">_get_metric</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">metric_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_func</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/classification_evaluation.html#ClassificationEvaluation._get_metric"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation._get_metric" title="Link to this definition"></a></dt>
<dd><p>Generic method to get the metric.</p>
<p>Computes the metric if it is not set.</p>
<p class="rubric">Notes</p>
<p>The zero_division=0 parameter makes the precision, recall and f1
metrics ignore samples for which no predicted label is given,
to prevent division by zero errors.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation._treat_none_values">
<span class="sig-name descname"><span class="pre">_treat_none_values</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Series</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Series</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/classification_evaluation.html#ClassificationEvaluation._treat_none_values"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation._treat_none_values" title="Link to this definition"></a></dt>
<dd><p>Replaces None values in the given Series by an empty string or 0.</p>
<p>Replaces None values in the given Series by an empty string or 0,
depending on the data type of the Series.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>labels</strong> (<em>Series</em>) – The Series possibly containing None values.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The Series with None values filled.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Series</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation.accuracy">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">accuracy</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation.accuracy" title="Link to this definition"></a></dt>
<dd><p>Gets the accuracy.</p>
<p>Gets the proportion of correctly predicted labels among all labels.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation.compute_metrics">
<span class="sig-name descname"><span class="pre">compute_metrics</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/classification_evaluation.html#ClassificationEvaluation.compute_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation.compute_metrics" title="Link to this definition"></a></dt>
<dd><p>Computes the metrics for the evaluation of the sentiments prediction.</p>
<p>Computes the metrics for the evaluation of the sentiments prediction
and stores them in the dictionary of the metrics property.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation.correct_labels">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">correct_labels</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Series</span></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation.correct_labels" title="Link to this definition"></a></dt>
<dd><p>Gets the correct labels.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation.name">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">name</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation.name" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation.predicted_labels">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">predicted_labels</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Series</span></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation.predicted_labels" title="Link to this definition"></a></dt>
<dd><p>Gets the predicted labels.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation.set_labels">
<span class="sig-name descname"><span class="pre">set_labels</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">correct_labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Series</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predicted_labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Series</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/classification_evaluation.html#ClassificationEvaluation.set_labels"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.classification_evaluation.ClassificationEvaluation.set_labels" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation">
<span id="sentimentanalysis-src-sentiment-analysis-evaluation-cross-strategy-prompt-evaluation-module"></span><h2>SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation module<a class="headerlink" href="#module-SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation" title="Link to this heading"></a></h2>
<section id="cross-strategy-prompt-evaluation-py">
<h3>cross_strategy_prompt_evaluation.py<a class="headerlink" href="#cross-strategy-prompt-evaluation-py" title="Link to this heading"></a></h3>
<p>Version 1.0, updated on 2025-01-02</p>
</section>
<dl class="py class">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation.CrossStrategyPromptEvaluation">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation.</span></span><span class="sig-name descname"><span class="pre">CrossStrategyPromptEvaluation</span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/cross_strategy_prompt_evaluation.html#CrossStrategyPromptEvaluation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation.CrossStrategyPromptEvaluation" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">PromptEvaluation</span></code></p>
<p>CrossStrategyPromptEvaluation class.</p>
<div class="graphviz"><img src="_images/inheritance-7366ec714d0036d1587f67a244b292761c95e465.png" alt="Inheritance diagram of SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation.CrossStrategyPromptEvaluation" usemap="#inheritance8129e9a238" class="inheritance graphviz" /></div>
<map id="inheritance8129e9a238" name="inheritance8129e9a238">
<area shape="rect" id="node1" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation.CrossStrategyPromptEvaluation" target="_top" title="CrossStrategyPromptEvaluation class." alt="" coords="340,5,567,31"/>
</map><dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation.CrossStrategyPromptEvaluation._add_to_chunk_col_map">
<span class="sig-name descname"><span class="pre">_add_to_chunk_col_map</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">col_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_col_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/cross_strategy_prompt_evaluation.html#CrossStrategyPromptEvaluation._add_to_chunk_col_map"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation.CrossStrategyPromptEvaluation._add_to_chunk_col_map" title="Link to this definition"></a></dt>
<dd><p>Adds or maps column names to a new target column name.</p>
<p>Handles the renaming of columns in ‘overall_chunk’ by adding entries to
‘chunk_col_map’ or processing duplicate column names.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col_names</strong> (<em>List</em><em>[</em><em>str</em><em>]</em>) – List of current column names to be mapped.</p></li>
<li><p><strong>target_col_name</strong> (<em>str</em>) – New column name to which the current names will be mapped.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – If more than two identical query column names are encountered.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation.CrossStrategyPromptEvaluation._align_query_nrs">
<span class="sig-name descname"><span class="pre">_align_query_nrs</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/cross_strategy_prompt_evaluation.html#CrossStrategyPromptEvaluation._align_query_nrs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation.CrossStrategyPromptEvaluation._align_query_nrs" title="Link to this definition"></a></dt>
<dd><p>Aligns query numbers across all best prompts.</p>
<p>Ensures that query numbers in the ‘overall_best_prompts`
and the corresponding columns in the ‘overall_chunk’ are renumbered
consecutively. The renumbering facilitates consistency and subsequent
analysis.</p>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>Updates ‘chunk_col_map’ to reflect the renumbered columns.</p></li>
<li><p>Renames and reorders columns in ‘overall_chunk’ to match the updated
query numbers</p></li>
</ul>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation.CrossStrategyPromptEvaluation._extract_best_prompts">
<span class="sig-name descname"><span class="pre">_extract_best_prompts</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">best_query_nrs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/cross_strategy_prompt_evaluation.html#CrossStrategyPromptEvaluation._extract_best_prompts"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation.CrossStrategyPromptEvaluation._extract_best_prompts" title="Link to this definition"></a></dt>
<dd><p>Extracts the specified best queries from all queries.</p>
<p>Extracts the specified best queries from all queries and assembles
them in the overall_best_prompts dictionary of this class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>best_query_nrs</strong> (<em>List</em><em>[</em><em>str</em><em>]</em>) – List of column names of the best prompts.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This method does not return the extracted queries. Instead,
it stores them in the overall_best_prompts dictionary of this
class.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation.CrossStrategyPromptEvaluation._handle_double_col_names_in_overall_chunk">
<span class="sig-name descname"><span class="pre">_handle_double_col_names_in_overall_chunk</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">col_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_col_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/cross_strategy_prompt_evaluation.html#CrossStrategyPromptEvaluation._handle_double_col_names_in_overall_chunk"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation.CrossStrategyPromptEvaluation._handle_double_col_names_in_overall_chunk" title="Link to this definition"></a></dt>
<dd><p>Resolves duplicate column names in the overall chunk.</p>
<p>Handles cases where the same query appears multiple times (e.g.,
as ‘_x’ and ‘_y’ suffixes) and renames or maps them accordingly. If
unexpected cases arise, applies a fallback renaming strategy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col_names</strong> (<em>List</em><em>[</em><em>str</em><em>]</em>) – List of duplicate column names.</p></li>
<li><p><strong>target_col_name</strong> (<em>str</em>) – Target column name to resolve the duplicates against.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>Renames columns in the ‘overall_chunk’ DataFrame to avoid conflicts.</p></li>
<li><p>Updates ‘chunk_col_map’ with resolved column mappings.</p></li>
<li><p>Applies a fallback strategy for unexpected cases.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="SentimentAnalysis.src.sentiment_analysis.retrieval.html#SentimentAnalysis.src.sentiment_analysis.retrieval.custom_exceptions.CriticalException" title="SentimentAnalysis.src.sentiment_analysis.retrieval.custom_exceptions.CriticalException"><strong>CriticalException</strong></a> – If an unresolvable case is encountered during renaming.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation.CrossStrategyPromptEvaluation._map_overall_chunk_columns">
<span class="sig-name descname"><span class="pre">_map_overall_chunk_columns</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">current_query_nr</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_query_nr</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/cross_strategy_prompt_evaluation.html#CrossStrategyPromptEvaluation._map_overall_chunk_columns"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation.CrossStrategyPromptEvaluation._map_overall_chunk_columns" title="Link to this definition"></a></dt>
<dd><p>Maps the current overall_chunk column name to a new column name.</p>
<p>Adds a current column name/target column name pair to the
chunk_col_map dictionary of this class. When completed, the dictionary
is used to replace the current column names by the target
column names.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>current_query_nr</strong> (<em>str</em>) – The current query identifier appended to a “<a href="#id1"><span class="problematic" id="id2">query_</span></a>” or “<a href="#id3"><span class="problematic" id="id4">answer_</span></a>”
column in the overall_chunk.</p></li>
<li><p><strong>target_query_nr</strong> (<em>str</em>) – The new query identifier by which to replace the current
identifier to build the new column name.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation.CrossStrategyPromptEvaluation._save_overall_best_prompt_ingredients_sets">
<span class="sig-name descname"><span class="pre">_save_overall_best_prompt_ingredients_sets</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/cross_strategy_prompt_evaluation.html#CrossStrategyPromptEvaluation._save_overall_best_prompt_ingredients_sets"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation.CrossStrategyPromptEvaluation._save_overall_best_prompt_ingredients_sets" title="Link to this definition"></a></dt>
<dd><p>Saves the overall best prompt ingredients sets.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation.CrossStrategyPromptEvaluation._save_overall_best_prompts">
<span class="sig-name descname"><span class="pre">_save_overall_best_prompts</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/cross_strategy_prompt_evaluation.html#CrossStrategyPromptEvaluation._save_overall_best_prompts"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation.CrossStrategyPromptEvaluation._save_overall_best_prompts" title="Link to this definition"></a></dt>
<dd><p>Saves the overall best prompts re-indexing the dictionary.</p>
<p class="rubric">Notes</p>
<p>“Re-indexes” the overall_best_prompts dictionary to match the
positional index of the elements in
overall_best_prompt_ingredients_sets.
The keys in the overall_best_prompts dictionary correspond to
the index numbers + 1 in the overall_best_prompt_ingredients list.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation.CrossStrategyPromptEvaluation._save_overall_chunk">
<span class="sig-name descname"><span class="pre">_save_overall_chunk</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/cross_strategy_prompt_evaluation.html#CrossStrategyPromptEvaluation._save_overall_chunk"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation.CrossStrategyPromptEvaluation._save_overall_chunk" title="Link to this definition"></a></dt>
<dd><p>Saves the combined chunk of best queries.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation.CrossStrategyPromptEvaluation.join_best_prompts">
<span class="sig-name descname"><span class="pre">join_best_prompts</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">versions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/cross_strategy_prompt_evaluation.html#CrossStrategyPromptEvaluation.join_best_prompts"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.cross_strategy_prompt_evaluation.CrossStrategyPromptEvaluation.join_best_prompts" title="Link to this definition"></a></dt>
<dd><p>Joins the best prompts from multiple prompt set versions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>versions</strong> (<em>List</em><em>[</em><em>str</em><em>]</em>) – List of versions the best prompts are to be collected from.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>The collected best prompts are not returned from this method but
saved so that they can be retrieved in later runs of the program.</p></li>
<li><p>The prompt engineering and optimization having been performed on
English samples only, the language targeted in this method is also
English.</p></li>
</ul>
</dd></dl>

</dd></dl>

</section>
<section id="module-SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation">
<span id="sentimentanalysis-src-sentiment-analysis-evaluation-deep-prompt-evaluation-module"></span><h2>SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation module<a class="headerlink" href="#module-SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation" title="Link to this heading"></a></h2>
<section id="deep-prompt-evaluation-py">
<h3>deep_prompt_evaluation.py<a class="headerlink" href="#deep-prompt-evaluation-py" title="Link to this heading"></a></h3>
<p>Version 1.0, updated on 2025-01-10</p>
</section>
<dl class="py class">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.</span></span><span class="sig-name descname"><span class="pre">DeepPromptEvaluation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="SentimentAnalysis.src.data_structures.html#SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame" title="SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame"><span class="pre">MyDataFrame</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">language</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'en'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/deep_prompt_evaluation.html#DeepPromptEvaluation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">MetricsVisualizationMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LoggingMixin</span></code></p>
<p>DeepPromptEvaluation class.</p>
<p>This class evaluates prompts by analyzing their components and computing
metrics.</p>
<div class="graphviz"><img src="_images/inheritance-79aec14829e45f29894e17b93946c130d3849754.png" alt="Inheritance diagram of SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation" usemap="#inheritanceb6703184ca" class="inheritance graphviz" /></div>
<map id="inheritanceb6703184ca" name="inheritanceb6703184ca">
<area shape="rect" id="node1" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation" target="_top" title="DeepPromptEvaluation class." alt="" coords="385,5,555,31"/>
</map><dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="SentimentAnalysis.src.data_structures.html#SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame" title="SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame"><span class="pre">MyDataFrame</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">language</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'en'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/deep_prompt_evaluation.html#DeepPromptEvaluation.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.__init__" title="Link to this definition"></a></dt>
<dd><p>Constructor.</p>
<p>Initializes the DeepPromptEvaluation class with the given parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="SentimentAnalysis.src.data_structures.html#SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame" title="SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame"><em>MyDataFrame</em></a>) – MyDataFrame instance containing a DataFrame with different
query columns and the corresponding answers retrieved from the
LLM’s API. These data are needed to evaluate the quality of the
different prompts.</p></li>
<li><p><strong>language</strong> (<em>str</em>) – Language code indicating the language of the sentences for which
the queries were formulated. Defaults to English (‘en’).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation._add_basic_ingredient">
<span class="sig-name descname"><span class="pre">_add_basic_ingredient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">df</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt_part_category</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt_part_value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">DataFrame</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/deep_prompt_evaluation.html#DeepPromptEvaluation._add_basic_ingredient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation._add_basic_ingredient" title="Link to this definition"></a></dt>
<dd><blockquote>
<div><p>Adds basic ingredients to the provided DataFrame.</p>
<p>For the given prompt part category, adds basic ingredients variantsto
the provided DataFrame.</p>
<dl class="simple">
<dt>df<span class="classifier">DataFrame</span></dt><dd><p>A DataFrame whose rows represent the prompts, with a rank
column containing the rank the prompt has received, and columns
representing basic ingredients like “before_sentence”,
“before_mention”.</p>
</dd>
<dt>prompt_name<span class="classifier">str</span></dt><dd><p>The prompt name (e.g. “en_1”) indicating the row in which to add
the ingredients derived from the prompt_part_value parameter.</p>
</dd>
<dt>prompt_part_category<span class="classifier">str</span></dt><dd><p>Prompt part category like “before_sentence” or “question” for
which to retrieve basic ingredients categories like
“sentence_label”, “politeness”, “toward” etc.</p>
</dd>
<dt>prompt_part_value<span class="classifier">str</span></dt><dd><p>Concrete prompt part in which to identify basic ingredients to
insert into the DataFrame, e.g. “Can you specifiy the opinion in
the statement targeted at the individual.</p>
</dd>
</dl>
</div></blockquote>
<dl>
<dt>Here is the</dt><dd><blockquote>
<div><p>statement: “.</p>
</div></blockquote>
<dl class="simple">
<dt>DataFrame</dt><dd><p>The DataFrame with the found basic ingredients added.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation._analyze_high_correlations">
<span class="sig-name descname"><span class="pre">_analyze_high_correlations</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">corr_matrix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.7</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">DataFrame</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/deep_prompt_evaluation.html#DeepPromptEvaluation._analyze_high_correlations"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation._analyze_high_correlations" title="Link to this definition"></a></dt>
<dd><p>Keeps only ingredients with high absolute correlation values.</p>
<p>Keeps only ingredients with absolutecorrelation values above the given
threshold.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>corr_matrix</strong> (<em>DataFrame</em>) – A Pandas DataFrame representing the correlation matrix.</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – The threshold for keeping ingredients with high correlation values.
Defaults to 0.7.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The reduced correlation matrix.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation._analyze_rank_correlations">
<span class="sig-name descname"><span class="pre">_analyze_rank_correlations</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">corr_matrix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">DataFrame</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/deep_prompt_evaluation.html#DeepPromptEvaluation._analyze_rank_correlations"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation._analyze_rank_correlations" title="Link to this definition"></a></dt>
<dd><p>Analyzes the rank correlations for specific prefixes in the matrix.</p>
<p>The method processes the input correlation matrix to extract
correlations of all parts of the prompt with the rank, excluding
self-correlation of the “rank” itself. It then utilizes unique
prefixes from the correlation indices to identify structured groups
of correlations. Finally, it sorts and visualizes the partial
correlations for each prefix using a heatmap.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>corr_matrix</strong> (<em>DataFrame</em>) – A Pandas DataFrame representing the correlation matrix. It must
contain a column named ‘rank’, which includes the correlations
of various prompt elements with the rank.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This method does not return a value; instead, it visualizes a
heatmap for specific partial rank correlations.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation._decompose_prompts">
<span class="sig-name descname"><span class="pre">_decompose_prompts</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/deep_prompt_evaluation.html#DeepPromptEvaluation._decompose_prompts"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation._decompose_prompts" title="Link to this definition"></a></dt>
<dd><p>Decomposes prompts into basic ingredients.</p>
<p>Sets the decomposed_prompts property.</p>
<p class="rubric">Notes</p>
<p>This implementation supposes there is only one basic ingredient per
prompt part.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation._encode_prompt_ingredients">
<span class="sig-name descname"><span class="pre">_encode_prompt_ingredients</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/deep_prompt_evaluation.html#DeepPromptEvaluation._encode_prompt_ingredients"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation._encode_prompt_ingredients" title="Link to this definition"></a></dt>
<dd><p>Applies one-hot encoding in preparation of the correlation analysis.</p>
<p>The resulting encoded prompt ingredients DataFrame is stored in the
corresponding property.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation._find_high_corr_rows">
<span class="sig-name descname"><span class="pre">_find_high_corr_rows</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">corr_matrix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.7</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/deep_prompt_evaluation.html#DeepPromptEvaluation._find_high_corr_rows"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation._find_high_corr_rows" title="Link to this definition"></a></dt>
<dd><p>Identifies rows with high correlation values (positive or negative)
in the given correlation matrix based on the specified threshold.
The function excludes self-correlations by masking diagonal elements
with NaN and then checks if any value in a row exceeds the threshold
in magnitude.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>corr_matrix</strong> (<em>DataFrame</em>) – The correlation matrix containing pairwise correlation values. It
is assumed to have the same index and column labels.</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – The threshold used to detect high correlations. Rows with any
value above this threshold or below -threshold (ignoring
self-correlations) will be identified. Default is 0.7.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A list of index labels corresponding to the rows in the
correlation matrix that have at least one correlation value
exceeding the specified threshold in magnitude.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>List[str]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation._get_basic_ingredients_categories_for_prompt_part_category">
<span class="sig-name descname"><span class="pre">_get_basic_ingredients_categories_for_prompt_part_category</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prompt_part_category</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/deep_prompt_evaluation.html#DeepPromptEvaluation._get_basic_ingredients_categories_for_prompt_part_category"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation._get_basic_ingredients_categories_for_prompt_part_category" title="Link to this definition"></a></dt>
<dd><p>Retrieves the basic ingredients categories for a prompt part category.</p>
<p>Retrieves the basic ingredients categories from which a prompt part
category is composed. Searches the prompt part category in the
basic_and_composed_ingredients property and returns the
corresponding list of basic ingredients categories.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>prompt_part_category</strong> (<em>str</em>) – Prompt part category like “before_sentence” or “question” for
which to retrieve basic ingredients categories like
“sentence_label”, “politeness”, “toward” etc.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The list of basic ingredients categories found for the prompt
part category.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>List[Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation._replace_language_specific_index">
<span class="sig-name descname"><span class="pre">_replace_language_specific_index</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">DataFrame</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Series</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">DataFrame</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Series</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/deep_prompt_evaluation.html#DeepPromptEvaluation._replace_language_specific_index"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation._replace_language_specific_index" title="Link to this definition"></a></dt>
<dd><p>Replaces the original language-specific index in a DataFrame or Series.</p>
<p>Replaces the language prefix with the overall prefix ‘prompt’</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.add_macro_ranks">
<span class="sig-name descname"><span class="pre">add_macro_ranks</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/deep_prompt_evaluation.html#DeepPromptEvaluation.add_macro_ranks"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.add_macro_ranks" title="Link to this definition"></a></dt>
<dd><p>Adds ranks to the metrics based on the macro metrics.</p>
<p class="rubric">Notes</p>
<p>The metrics DataFrame is changed in place. The resulting DataFrame
can be retrieved using the metrics getter of this class.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.add_prompt_freqs">
<span class="sig-name descname"><span class="pre">add_prompt_freqs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">evaluation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation.SinglePromptEvaluation" title="SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation.SinglePromptEvaluation"><span class="pre">SinglePromptEvaluation</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/deep_prompt_evaluation.html#DeepPromptEvaluation.add_prompt_freqs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.add_prompt_freqs" title="Link to this definition"></a></dt>
<dd><p>Adds sentiment frequencies for a prompt to the all_freqs dictionary.</p>
<p>Adds the sentiment frequencies for a prompt to the overall dictionary
of sentiment frequencies of all prompts.</p>
<p>Retrieves the frequencies for a single prompt from the
SinglePromptEvaluation instance created for the prompt’s evaluation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>evaluation</strong> (<a class="reference internal" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation.SinglePromptEvaluation" title="SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation.SinglePromptEvaluation"><em>SinglePromptEvaluation</em></a>) – The SinglePromptEvaluation instance created for the prompt’s
evaluation.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.add_prompt_metrics">
<span class="sig-name descname"><span class="pre">add_prompt_metrics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">evaluation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation.SinglePromptEvaluation" title="SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation.SinglePromptEvaluation"><span class="pre">SinglePromptEvaluation</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/deep_prompt_evaluation.html#DeepPromptEvaluation.add_prompt_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.add_prompt_metrics" title="Link to this definition"></a></dt>
<dd><p>Adds computed metrics for a prompt to the overall metrics MyDataFrame.</p>
<p>The overall metrics MyDataFrame collects the metrics for all prompts.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>evaluation</strong> (<a class="reference internal" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation.SinglePromptEvaluation" title="SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation.SinglePromptEvaluation"><em>SinglePromptEvaluation</em></a>) – A SinglePromptEvaluation instance for the currently analyzed
prompt that computes the prompt’s metrics.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.add_ranks">
<span class="sig-name descname"><span class="pre">add_ranks</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/deep_prompt_evaluation.html#DeepPromptEvaluation.add_ranks"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.add_ranks" title="Link to this definition"></a></dt>
<dd><p>Adds ranks to the metrics based on the macro metrics.</p>
<p class="rubric">Notes</p>
<p>The metrics DataFrame is changed in place. The resulting DataFrame
can be retrieved using the metrics getter of this class.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.all_freqs">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">all_freqs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.all_freqs" title="Link to this definition"></a></dt>
<dd><p>Returns the dictionary of sentiment frequencies for all prompts.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dictionary where the keys are the names of the prompts and the
values are lists of three tuples for the three sentiment
classes where the first element constitutes the sentiment label
and the second element is the frequency value.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>Dict[str, List[Tuple[str, int]]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.all_freqs_df">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">all_freqs_df</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">DataFrame</span></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.all_freqs_df" title="Link to this definition"></a></dt>
<dd><p>Returns a DataFrame with the content of the all_freqs dictionary.</p>
<p>Returns the DataFrame from the all_freqs_my_df MyDataFrame object
created from the all_freqs dictionary..</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.all_freqs_my_df">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">all_freqs_my_df</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="SentimentAnalysis.src.data_structures.html#SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame" title="SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame"><span class="pre">MyDataFrame</span></a></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.all_freqs_my_df" title="Link to this definition"></a></dt>
<dd><p>Creates and returns a MyDataFrame from the all_freqs dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The created MyDataFrame with the content of the all_freqs
dictionary.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="SentimentAnalysis.src.data_structures.html#SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame" title="SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame">MyDataFrame</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.all_freqs_my_df_with_totals">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">all_freqs_my_df_with_totals</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="SentimentAnalysis.src.data_structures.html#SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame" title="SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame"><span class="pre">MyDataFrame</span></a></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.all_freqs_my_df_with_totals" title="Link to this definition"></a></dt>
<dd><p>Returns the all_freqs_my_df MyDataFrame with a column for the totals.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The all_freqs_my_df MyDataFrame with a column for the totals added.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="SentimentAnalysis.src.data_structures.html#SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame" title="SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame">MyDataFrame</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.analyze_correlation">
<span class="sig-name descname"><span class="pre">analyze_correlation</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/deep_prompt_evaluation.html#DeepPromptEvaluation.analyze_correlation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.analyze_correlation" title="Link to this definition"></a></dt>
<dd><p>Analyzes the correlations between all prompt parts.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.best">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">best</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="SentimentAnalysis.src.data_structures.html#SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame" title="SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame"><span class="pre">MyDataFrame</span></a></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.best" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.cols_to_analyze">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">cols_to_analyze</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.cols_to_analyze" title="Link to this definition"></a></dt>
<dd><p>Retrieves all names of ‘answer’ columns from the data chunk.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A list of names of columns in the data chunk that constitute
‘answer’ columns.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>List[str]</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The name of an ‘answer’ column starts with “<a href="#id5"><span class="problematic" id="id6">answer_</span></a>” followed by a
prompt number that corresponds to one of the numbers used to
identify the query columns in the chunk.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.compute_all_prompt_metrics">
<span class="sig-name descname"><span class="pre">compute_all_prompt_metrics</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/deep_prompt_evaluation.html#DeepPromptEvaluation.compute_all_prompt_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.compute_all_prompt_metrics" title="Link to this definition"></a></dt>
<dd><p>Computes the metrics for all columns to analyze
Instantiates a SinglePromptEvaluation instance for each column that
is to be analyzed and adds prompt metrics and frequencies to
collections of metrics and frequencies for all prompts.</p>
<p>Shows the overall frequencies after having added the sentiment
frequencies for all prompts.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.correct_labels">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">correct_labels</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="SentimentAnalysis.src.stats.html#SentimentAnalysis.src.stats.labels.Labels" title="SentimentAnalysis.src.stats.labels.Labels"><span class="pre">Labels</span></a></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.correct_labels" title="Link to this definition"></a></dt>
<dd><p>Gets the correct labels.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.data">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">data</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Chunk</span></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.data" title="Link to this definition"></a></dt>
<dd><p>Returns the data chunk.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The data chunk Chunk with samples, query and answer columns.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>Chunk</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.decomposed_prompts">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">decomposed_prompts</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">DataFrame</span></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.decomposed_prompts" title="Link to this definition"></a></dt>
<dd><p>Returns the decomposed prompts.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The DataFrame with the decomposed prompts and their ranks.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.describe_answer_data">
<span class="sig-name descname"><span class="pre">describe_answer_data</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/deep_prompt_evaluation.html#DeepPromptEvaluation.describe_answer_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.describe_answer_data" title="Link to this definition"></a></dt>
<dd><p>Describes the answer data columns with the pandas describe() function.</p>
<p>Filters the DataFrame wrapped in the data MyDataFrame object for
answer columns to the console and prints the filtered DataFrame to the
console.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.describe_data">
<span class="sig-name descname"><span class="pre">describe_data</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/deep_prompt_evaluation.html#DeepPromptEvaluation.describe_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.describe_data" title="Link to this definition"></a></dt>
<dd><p>Describes the data with the pandas describe() function.</p>
<p>Prints the DataFrame wrapped in the data MyDataFrame object to the
console.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.encoded_prompt_ingredients">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">encoded_prompt_ingredients</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">DataFrame</span></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.encoded_prompt_ingredients" title="Link to this definition"></a></dt>
<dd><p>Returns the one-hot-encoded prompt ingredients.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The one-hot-encoded prompt ingredients.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.get_metrics_for_aggregation">
<span class="sig-name descname"><span class="pre">get_metrics_for_aggregation</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">DataFrame</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/deep_prompt_evaluation.html#DeepPromptEvaluation.get_metrics_for_aggregation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.get_metrics_for_aggregation" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.get_partial_metrics">
<span class="sig-name descname"><span class="pre">get_partial_metrics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">col_substring</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="SentimentAnalysis.src.data_structures.html#SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame" title="SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame"><span class="pre">MyDataFrame</span></a></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/deep_prompt_evaluation.html#DeepPromptEvaluation.get_partial_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.get_partial_metrics" title="Link to this definition"></a></dt>
<dd><p>Extracts partial metrics based on the given column name substring.</p>
<p>Extracts columns from the metrics DataFrame whose column names
contain the given substring.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>col_substring</strong> (<em>str</em>) – The substring to match column names with.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A MyDataFrame object containing the extracted columns.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="SentimentAnalysis.src.data_structures.html#SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame" title="SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame">MyDataFrame</a></p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If the columns to extract are macro metrics, the accuracy (‘acc’)
column is also added to the returned MyDataFrame as it is also an
overall metric.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.get_ranking">
<span class="sig-name descname"><span class="pre">get_ranking</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">replace_language_prefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Series</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/deep_prompt_evaluation.html#DeepPromptEvaluation.get_ranking"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.get_ranking" title="Link to this definition"></a></dt>
<dd><p>Returns the ranking replacing the language prefix in the row index.</p>
<p>Replaces the language prefix in the row names by a prefix that just
identifies the respective prompt.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>replace_language_prefix</strong> (<em>bool</em>) – If set to True, the prefix replacement method is called.
Otherwise, the row names of the ranking Series remain unchanged.
Defaults to False.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The ranking Series with the new index (if this is replaced) or
the old index (if the replace_language_prefix was not set to True.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Series</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.metrics">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">metrics</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="SentimentAnalysis.src.data_structures.html#SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame" title="SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame"><span class="pre">MyDataFrame</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.metrics" title="Link to this definition"></a></dt>
<dd><p>Returns the metrics MyDataFrame.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A MyDataFrame containing a DataFrame with the metrics where the
metrics names are column names and the query variants rows with
metrics values.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="SentimentAnalysis.src.data_structures.html#SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame" title="SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame">MyDataFrame</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.metrics_are_equal">
<span class="sig-name descname"><span class="pre">metrics_are_equal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">metric_1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/deep_prompt_evaluation.html#DeepPromptEvaluation.metrics_are_equal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.metrics_are_equal" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.n_prompts">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">n_prompts</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.n_prompts" title="Link to this definition"></a></dt>
<dd><p>Returns the number of best (and worst) prompts to show.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The the number of best (and worst) prompts to show.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.ranking">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">ranking</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Series</span></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.ranking" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.worst">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">worst</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="SentimentAnalysis.src.data_structures.html#SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame" title="SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame"><span class="pre">MyDataFrame</span></a></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation.worst" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-SentimentAnalysis.src.sentiment_analysis.evaluation.evaluation_workflow">
<span id="sentimentanalysis-src-sentiment-analysis-evaluation-evaluation-workflow-module"></span><h2>SentimentAnalysis.src.sentiment_analysis.evaluation.evaluation_workflow module<a class="headerlink" href="#module-SentimentAnalysis.src.sentiment_analysis.evaluation.evaluation_workflow" title="Link to this heading"></a></h2>
<section id="evaluation-workflow-py">
<h3>evaluation_workflow.py<a class="headerlink" href="#evaluation-workflow-py" title="Link to this heading"></a></h3>
<p>Version 1.0, updated on 2025-01-02</p>
</section>
<dl class="py class">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.evaluation_workflow.EvaluationWorkflow">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">SentimentAnalysis.src.sentiment_analysis.evaluation.evaluation_workflow.</span></span><span class="sig-name descname"><span class="pre">EvaluationWorkflow</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">strategy_nr</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/evaluation_workflow.html#EvaluationWorkflow"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.evaluation_workflow.EvaluationWorkflow" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>EvaluationWorkflow class</p>
<div class="graphviz"><img src="_images/inheritance-742fb350da82830302652b93de3908b3bcc04732.png" alt="Inheritance diagram of SentimentAnalysis.src.sentiment_analysis.evaluation.evaluation_workflow.EvaluationWorkflow" usemap="#inheritance9fe5d79fb2" class="inheritance graphviz" /></div>
<map id="inheritance9fe5d79fb2" name="inheritance9fe5d79fb2">
<area shape="rect" id="node1" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.evaluation_workflow.EvaluationWorkflow" target="_top" title="EvaluationWorkflow class" alt="" coords="5,5,148,31"/>
</map><dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.evaluation_workflow.EvaluationWorkflow.evaluate_cross_lingual_performance">
<span class="sig-name descname"><span class="pre">evaluate_cross_lingual_performance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/evaluation_workflow.html#EvaluationWorkflow.evaluate_cross_lingual_performance"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.evaluation_workflow.EvaluationWorkflow.evaluate_cross_lingual_performance" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.evaluation_workflow.EvaluationWorkflow.evaluate_language">
<span class="sig-name descname"><span class="pre">evaluate_language</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">language</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'en'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/evaluation_workflow.html#EvaluationWorkflow.evaluate_language"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.evaluation_workflow.EvaluationWorkflow.evaluate_language" title="Link to this definition"></a></dt>
<dd><p>Evaluates the prompts of the specified language.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>language</strong> (<em>str</em>) – The language of the prompts to evaluate.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Ensure you have moved the generated chunks files in the csv data
folder to a subfolder named “<a href="#id7"><span class="problematic" id="id8">chunks_v_</span></a>” + the two-digit number of
the prompt engineering strategy it was created by (e.g. “chunks_v_01”)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.evaluation_workflow.EvaluationWorkflow.evaluate_prompt_group">
<span class="sig-name descname"><span class="pre">evaluate_prompt_group</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prompt_group</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">language</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'en'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/evaluation_workflow.html#EvaluationWorkflow.evaluate_prompt_group"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.evaluation_workflow.EvaluationWorkflow.evaluate_prompt_group" title="Link to this definition"></a></dt>
<dd><p>Evaluates the given group of prompts in the specified language context.</p>
<p>Configures the evaluation environment by setting the specified
language and performs an evaluation on the provided group of prompts.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prompt_group</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) – A list of integers representing the group of prompts to be
evaluated.</p></li>
<li><p><strong>language</strong> (<em>str</em>) – The language setting for the evaluation, given as a string.
Default is ‘en’.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>This method does not return any value but outputs results to the
console.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="sentimentanalysis-src-sentiment-analysis-evaluation-language-results-processor-module">
<h2>SentimentAnalysis.src.sentiment_analysis.evaluation.language_results_processor module<a class="headerlink" href="#sentimentanalysis-src-sentiment-analysis-evaluation-language-results-processor-module" title="Link to this heading"></a></h2>
<section id="language-results-processor-py">
<h3>language_results_processor.py<a class="headerlink" href="#language-results-processor-py" title="Link to this heading"></a></h3>
<p>Version 1.0, updated on 2025-01-01</p>
</section>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">SentimentAnalysis.src.sentiment_analysis.evaluation.language_results_processor.</span></span><span class="sig-name descname"><span class="pre">LanguageResultsProcessor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">language</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/language_results_processor.html#LanguageResultsProcessor"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">LoggingMixin</span></code></p>
<p>LanguageResultsProcessor class.</p>
<p>This class processes managing and processing sentiment analysis results
for a given language. It evaluates metrics, analyzes prompts, and
facilitates comparison of results.</p>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">language</span></span></dt>
<dd><p>The language being processed (e.g., “en” for English).</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="simple">
<dt>config<span class="classifier">SentimentAnalysisConfig</span></dt><dd><p>Configuration settings for sentiment analysis and associated tasks
(prompt engineering, evaluation of predictions, etc.)</p>
</dd>
<dt>metrics<span class="classifier">MyDataFrame</span></dt><dd><p>The metrics for the current language.</p>
</dd>
<dt>data<span class="classifier">Chunk</span></dt><dd><p>The data with the batch samples and query and answer columns.</p>
</dd>
<dt>evaluation<span class="classifier">DeepPromptEvaluation</span></dt><dd><p>A DeepPromptEvaluation instance, providing detailed prompt evaluation.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">analyze_prompts</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">partial_metrics</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None:</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/language_results_processor.html#LanguageResultsProcessor.analyze_prompts"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Analyzes the prompts.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">get_best_query_nrs</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List[str]:</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/language_results_processor.html#LanguageResultsProcessor.get_best_query_nrs"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Retrieves the indices of the best-performing queries.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">process_language</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None:</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/language_results_processor.html#LanguageResultsProcessor.process_language"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Processes the language results.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">show_best_prompts</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">partial_metrics</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None:</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/language_results_processor.html#LanguageResultsProcessor.show_best_prompts"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Displays the best-performing prompts based on the evaluation metrics.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">show_partial_metrics(partial_metrics:</span> <span class="pre">List[str],</span> <span class="pre">show_best:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False,</span></span></dt>
<dd><blockquote>
<div><p>show_worst: bool = False) -&gt; None:</p>
</div></blockquote>
<p>Displays specified metrics for the best and/or worst prompts.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">show_worst_prompts</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">partial_metrics</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None:</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/language_results_processor.html#LanguageResultsProcessor.show_worst_prompts"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Displays the worst-performing prompts based on the evaluation metrics.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">verify_metrics_are_equal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">metric_1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool:</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/language_results_processor.html#LanguageResultsProcessor.verify_metrics_are_equal"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Compares two metrics and verifies if they are equal.</p>
</dd></dl>

<div class="graphviz"><img src="_images/inheritance-23121036b724fc292adf94463eb17d77dfe0b01b.png" alt="Inheritance diagram of SentimentAnalysis.src.sentiment_analysis.evaluation.language_results_processor.LanguageResultsProcessor" class="inheritance graphviz" /></div>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">language</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/language_results_processor.html#LanguageResultsProcessor.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Constructor.</p>
<p>Initializes the LanguageResultsProcessor class with the provided
language.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>language</strong> (<em>str</em>) – The language to process.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">_get_data_from_chunks</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Chunk</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/language_results_processor.html#LanguageResultsProcessor._get_data_from_chunks"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Uses the ChunKLoader to retrieve a valid queries chunk.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A chunk object with valid queries.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>Chunk</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">analyze_prompts</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">partial_metrics</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/language_results_processor.html#LanguageResultsProcessor.analyze_prompts"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Analyzes the prompts.</p>
<p>Finds best and worst prompts and correlations of prompt ingredients
with valid/invalid prompts if there are enough different prompts to
perform such an analysis.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>partial_metrics</strong> (<em>List</em><em>[</em><em>str</em><em>]</em>) – List of partial metrics to show for the best and worst prompts.
Defaults to None. If no partial metrics are provided,
the show_best_prompts and show_worst_prompts methods will just
show the best and worst prompt overall metrics values but will not
display any partial metrics diagrams.</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">answer_cols</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">data</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Chunk</span></em></dt>
<dd><p>Retrieves the data with the batch samples and query and answer columns.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A chunk object containing the data.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>Chunk</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">evaluation</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation" title="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation"><span class="pre">DeepPromptEvaluation</span></a></em></dt>
<dd><p>Provides access to the evaluation instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The evaluation instance for analyzing metrics and prompts.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation" title="SentimentAnalysis.src.sentiment_analysis.evaluation.deep_prompt_evaluation.DeepPromptEvaluation">DeepPromptEvaluation</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">get_best_query_nrs</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/language_results_processor.html#LanguageResultsProcessor.get_best_query_nrs"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Retrieves the indices of the best-performing queries.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A list of indices corresponding to the best queries.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>List[str]</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">metrics</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="SentimentAnalysis.src.data_structures.html#SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame" title="SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame"><span class="pre">MyDataFrame</span></a></em></dt>
<dd><p>Returns the metrics for the current language.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A MyDataFrame instance containing the sorted metrics DataFrame.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="SentimentAnalysis.src.data_structures.html#SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame" title="SentimentAnalysis.src.data_structures.my_data_frame.MyDataFrame">MyDataFrame</a></p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The metrics DataFrame is sorted in ascending order (from best to
worst rank) by the ranks of the prompts.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">process_language</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/language_results_processor.html#LanguageResultsProcessor.process_language"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Processes the language results.</p>
<p>Processes the language results by loading data chunks, creating an
evaluation instance, and computing sorted metrics.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">show_best_prompts</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">partial_metrics</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/language_results_processor.html#LanguageResultsProcessor.show_best_prompts"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Displays the best-performing prompts based on the evaluation metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>partial_metrics</strong> (<em>List</em><em>[</em><em>str</em><em>] </em><em>| </em><em>None</em>) – A list of specific metrics to display for the best prompts. If
None, only overall metrics are shown.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">show_partial_metrics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">partial_metrics</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show_best</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show_worst</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/language_results_processor.html#LanguageResultsProcessor.show_partial_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Displays specified metrics for the best and/or worst prompts.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>partial_metrics</strong> (<em>List</em><em>[</em><em>str</em><em>]</em>) – A list of metrics to display.</p></li>
<li><p><strong>show_best</strong> (<em>bool</em>) – Whether to show the metrics for the best prompts.</p></li>
<li><p><strong>show_worst</strong> (<em>bool</em>) – Whether to show the metrics for the worst prompts.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">show_worst_prompts</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">partial_metrics</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/language_results_processor.html#LanguageResultsProcessor.show_worst_prompts"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Displays the worst-performing prompts based on the evaluation metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>partial_metrics</strong> (<em>List</em><em>[</em><em>str</em><em>] </em><em>| </em><em>None</em>) – A list of specific metrics to display for the worst prompts. If
None, only overall metrics are shown.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">verify_metrics_are_equal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">metric_1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/language_results_processor.html#LanguageResultsProcessor.verify_metrics_are_equal"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Compares two metrics and verifies if they are equal.</p>
<p>This method can be used to check two metrics that are supposed to be
identical.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>metric_1</strong> (<em>str</em>) – Name of the first metric. Should equal the column name of the
respective metric values in the metrics DataFrame.</p></li>
<li><p><strong>metric_2</strong> (<em>str</em>) – Name of the second metric to compare. Should equal the column
name of the respective metric values in the metrics DataFrame.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>True if the values of the metrics are equal, False otherwise.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-SentimentAnalysis.src.sentiment_analysis.evaluation.metrics">
<span id="sentimentanalysis-src-sentiment-analysis-evaluation-metrics-module"></span><h2>SentimentAnalysis.src.sentiment_analysis.evaluation.metrics module<a class="headerlink" href="#module-SentimentAnalysis.src.sentiment_analysis.evaluation.metrics" title="Link to this heading"></a></h2>
<section id="metrics-py">
<h3>metrics.py<a class="headerlink" href="#metrics-py" title="Link to this heading"></a></h3>
<p>Version 1.0, updated on 2024-12-20</p>
</section>
<dl class="py class">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.metrics.Metrics">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">SentimentAnalysis.src.sentiment_analysis.evaluation.metrics.</span></span><span class="sig-name descname"><span class="pre">Metrics</span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/metrics.html#Metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.metrics.Metrics" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Metrics class.</p>
<p>Provides a dictionary in which to store the metrics for the evaluation
of the results.</p>
<div class="graphviz"><img src="_images/inheritance-5fd94ef90a5a96df728faf9d0dde94b8391ace33.png" alt="Inheritance diagram of SentimentAnalysis.src.sentiment_analysis.evaluation.metrics.Metrics" usemap="#inheritance5f0ac47218" class="inheritance graphviz" /></div>
<map id="inheritance5f0ac47218" name="inheritance5f0ac47218">
<area shape="rect" id="node1" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.metrics.Metrics" target="_top" title="Metrics class." alt="" coords="5,5,77,31"/>
</map><dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.metrics.Metrics.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/metrics.html#Metrics.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.metrics.Metrics.__init__" title="Link to this definition"></a></dt>
<dd><p>Initializes the class by setting up a dictionary for the different
metrics.</p>
<p>Each metric is given an initial value which will be overwritten when
the real values are stored in the dictionary.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.metrics.Metrics.store">
<span class="sig-name descname"><span class="pre">store</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/metrics.html#Metrics.store"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.metrics.Metrics.store" title="Link to this definition"></a></dt>
<dd><p>Stores the specified value of the specified metric in the dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>metric</strong> (<em>str</em>) – The name of the metric whose value is to be stored.</p></li>
<li><p><strong>value</strong> (<em>flaat</em>) – The metric’s value.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-SentimentAnalysis.src.sentiment_analysis.evaluation.metrics_visualization_mixin">
<span id="sentimentanalysis-src-sentiment-analysis-evaluation-metrics-visualization-mixin-module"></span><h2>SentimentAnalysis.src.sentiment_analysis.evaluation.metrics_visualization_mixin module<a class="headerlink" href="#module-SentimentAnalysis.src.sentiment_analysis.evaluation.metrics_visualization_mixin" title="Link to this heading"></a></h2>
<section id="metrics-visualization-mixin-py">
<h3>metrics_visualization_mixin.py<a class="headerlink" href="#metrics-visualization-mixin-py" title="Link to this heading"></a></h3>
<p>Version 1.0, updated on 2025-01-02</p>
</section>
<dl class="py class">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.metrics_visualization_mixin.MetricsVisualizationMixin">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">SentimentAnalysis.src.sentiment_analysis.evaluation.metrics_visualization_mixin.</span></span><span class="sig-name descname"><span class="pre">MetricsVisualizationMixin</span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/metrics_visualization_mixin.html#MetricsVisualizationMixin"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.metrics_visualization_mixin.MetricsVisualizationMixin" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">LoggingMixin</span></code></p>
<div class="graphviz"><img src="_images/inheritance-6ef6bbd2b73b7b7fca28e0903909b276614d0a4f.png" alt="Inheritance diagram of SentimentAnalysis.src.sentiment_analysis.evaluation.metrics_visualization_mixin.MetricsVisualizationMixin" usemap="#inheritance7ddf0b4f61" class="inheritance graphviz" /></div>
<map id="inheritance7ddf0b4f61" name="inheritance7ddf0b4f61">
<area shape="rect" id="node2" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.metrics_visualization_mixin.MetricsVisualizationMixin" target="_top" title="MetricsVisualizationMixin" alt="" coords="157,5,337,31"/>
</map><dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.metrics_visualization_mixin.MetricsVisualizationMixin.diagram">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">diagram</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="SentimentAnalysis.src.stats.visualization.html#SentimentAnalysis.src.stats.visualization.diagram.Diagram" title="SentimentAnalysis.src.stats.visualization.diagram.Diagram"><span class="pre">Diagram</span></a></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.metrics_visualization_mixin.MetricsVisualizationMixin.diagram" title="Link to this definition"></a></dt>
<dd><p>Gets an instance of the Diagram class.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.metrics_visualization_mixin.MetricsVisualizationMixin.get_best">
<span class="sig-name descname"><span class="pre">get_best</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">DataFrame</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/metrics_visualization_mixin.html#MetricsVisualizationMixin.get_best"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.metrics_visualization_mixin.MetricsVisualizationMixin.get_best" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.metrics_visualization_mixin.MetricsVisualizationMixin.show_best">
<span class="sig-name descname"><span class="pre">show_best</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/metrics_visualization_mixin.html#MetricsVisualizationMixin.show_best"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.metrics_visualization_mixin.MetricsVisualizationMixin.show_best" title="Link to this definition"></a></dt>
<dd><p>Displays a DataFrame containing the best queries.</p>
<p>Prints the DataFrame in a formatted box in the console.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.metrics_visualization_mixin.MetricsVisualizationMixin.show_correlation_heatmap">
<span class="sig-name descname"><span class="pre">show_correlation_heatmap</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">correlation_data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Series</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">DataFrame</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/metrics_visualization_mixin.html#MetricsVisualizationMixin.show_correlation_heatmap"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.metrics_visualization_mixin.MetricsVisualizationMixin.show_correlation_heatmap" title="Link to this definition"></a></dt>
<dd><p>Displays a correlation heatmap using the provided correlation data.</p>
<p>This method generates a heatmap based on the input data to visually
represent correlation values between variables. It supports both
pandas Series and pandas DataFrame objects. If a Series is provided,
it uses a single dimension to plot the heatmap. If a DataFrame is
given, it processes the data to construct a multi-dimensional heatmap.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>correlation_data</strong> (<em>Series</em><em> or </em><em>DataFrame</em>) – The data containing correlation values. Accepts either a pandas
Series, which represents one-dimensional data, or a pandas
DataFrame for multi-dimensional correlation values.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This method does not return any values. The methods it calls directly
display the generated heatmap.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.metrics_visualization_mixin.MetricsVisualizationMixin.show_partial_metric">
<span class="sig-name descname"><span class="pre">show_partial_metric</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'macro'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show_best</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show_worst</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/metrics_visualization_mixin.html#MetricsVisualizationMixin.show_partial_metric"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.metrics_visualization_mixin.MetricsVisualizationMixin.show_partial_metric" title="Link to this definition"></a></dt>
<dd><p>Shows a diagram for the specified group of metrics.</p>
<p>The possible groups are defined by the submetrics property of this
class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>metric</strong> (<em>str</em>) – The group of metrics to show corresponding to one of the
submetrics defined in the list returned by the submetrics
property of this class.</p></li>
<li><p><strong>show_best</strong> (<em>bool</em>) – Whether to show the best queries. Defaults to False.</p></li>
<li><p><strong>show_worst</strong> (<em>bool</em>) – Whether to show the worst queries. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="SentimentAnalysis.src.sentiment_analysis.retrieval.html#SentimentAnalysis.src.sentiment_analysis.retrieval.custom_exceptions.CriticalException" title="SentimentAnalysis.src.sentiment_analysis.retrieval.custom_exceptions.CriticalException"><strong>CriticalException</strong></a> – If the given metric is not contained in the submetrics list.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.metrics_visualization_mixin.MetricsVisualizationMixin.show_worst">
<span class="sig-name descname"><span class="pre">show_worst</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/metrics_visualization_mixin.html#MetricsVisualizationMixin.show_worst"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.metrics_visualization_mixin.MetricsVisualizationMixin.show_worst" title="Link to this definition"></a></dt>
<dd><p>Displays a DataFrame containing the worst queries.</p>
<p>Prints the DataFrame in a formatted box in the console.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.metrics_visualization_mixin.MetricsVisualizationMixin.submetrics">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">submetrics</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.metrics_visualization_mixin.MetricsVisualizationMixin.submetrics" title="Link to this definition"></a></dt>
<dd><p>Gets the list of available submetrics.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.metrics_visualization_mixin.MetricsVisualizationMixin.thresholds">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">thresholds</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.metrics_visualization_mixin.MetricsVisualizationMixin.thresholds" title="Link to this definition"></a></dt>
<dd><p>Gets the thresholds for the different metrics.</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-SentimentAnalysis.src.sentiment_analysis.evaluation.optimization_workflow">
<span id="sentimentanalysis-src-sentiment-analysis-evaluation-optimization-workflow-module"></span><h2>SentimentAnalysis.src.sentiment_analysis.evaluation.optimization_workflow module<a class="headerlink" href="#module-SentimentAnalysis.src.sentiment_analysis.evaluation.optimization_workflow" title="Link to this heading"></a></h2>
<section id="optimization-workflow-py">
<h3>optimization_workflow.py<a class="headerlink" href="#optimization-workflow-py" title="Link to this heading"></a></h3>
<p>Version 1.0, updated on 2025-05-01</p>
</section>
<dl class="py class">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.optimization_workflow.OptimizationWorkflow">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">SentimentAnalysis.src.sentiment_analysis.evaluation.optimization_workflow.</span></span><span class="sig-name descname"><span class="pre">OptimizationWorkflow</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">strategy_nr</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/optimization_workflow.html#OptimizationWorkflow"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.optimization_workflow.OptimizationWorkflow" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>OptimizationWorkflow class</p>
<div class="graphviz"><img src="_images/inheritance-29981a5c8530b1a4466b0a1ba410a015ea8c5995.png" alt="Inheritance diagram of SentimentAnalysis.src.sentiment_analysis.evaluation.optimization_workflow.OptimizationWorkflow" usemap="#inheritance9a24e15dad" class="inheritance graphviz" /></div>
<map id="inheritance9a24e15dad" name="inheritance9a24e15dad">
<area shape="rect" id="node1" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.optimization_workflow.OptimizationWorkflow" target="_top" title="OptimizationWorkflow class" alt="" coords="5,5,164,31"/>
</map><dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.optimization_workflow.OptimizationWorkflow.find_optimization_potential_for_language">
<span class="sig-name descname"><span class="pre">find_optimization_potential_for_language</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">language</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/optimization_workflow.html#OptimizationWorkflow.find_optimization_potential_for_language"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.optimization_workflow.OptimizationWorkflow.find_optimization_potential_for_language" title="Link to this definition"></a></dt>
<dd><p>Finds optimization potentials for prompts for the given language.</p>
<p>This method evaluates the results of the sentiment predictions
obtained with the different prompts in terms of valid and invalid
prompt variants and prompt ingredients in order to know which
ingredients should be eliminated and which ones should be
preferred for better results.</p>
<p class="rubric">Notes</p>
<p>This method does not return anything. Instead, it prints the results
in the console.</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation">
<span id="sentimentanalysis-src-sentiment-analysis-evaluation-prompt-evaluation-module"></span><h2>SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation module<a class="headerlink" href="#module-SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation" title="Link to this heading"></a></h2>
<section id="prompt-evaluation-py">
<h3>prompt_evaluation.py<a class="headerlink" href="#prompt-evaluation-py" title="Link to this heading"></a></h3>
<p>Version 1.0, updated on 2025-02-05</p>
</section>
<dl class="py class">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.</span></span><span class="sig-name descname"><span class="pre">PromptEvaluation</span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/prompt_evaluation.html#PromptEvaluation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">LoggingMixin</span></code></p>
<p>PromptEvaluation class.</p>
<p>This class initializes and manages objects required for the evaluation of
sentiment analysis results. It provides logging capabilities through the
‘LoggingMixin’ class.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation.config">
<span class="sig-name descname"><span class="pre">config</span></span><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation.config" title="Link to this definition"></a></dt>
<dd><p>The sentiment analysis configuration object that holds configuration
values and settings for the analysis process.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>SentimentAnalysisConfig</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation.prompt_engineer">
<span class="sig-name descname"><span class="pre">prompt_engineer</span></span><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation.prompt_engineer" title="Link to this definition"></a></dt>
<dd><p>The prompt engineer instance responsible for generating prompt
configurations based on the provided version.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="SentimentAnalysis.src.sentiment_analysis.prompt_engineering.html#SentimentAnalysis.src.sentiment_analysis.prompt_engineering.prompt_engineer.PromptEngineer" title="SentimentAnalysis.src.sentiment_analysis.prompt_engineering.prompt_engineer.PromptEngineer">PromptEngineer</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation.prompt_ingredients_sets">
<span class="sig-name descname"><span class="pre">prompt_ingredients_sets</span></span><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation.prompt_ingredients_sets" title="Link to this definition"></a></dt>
<dd><p>The collection of prompt ingredient sets generated by the prompt
engineer.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>PromptsDictType</p>
</dd>
</dl>
</dd></dl>

<div class="graphviz"><img src="_images/inheritance-491881d234d3da310dec5bf437b108d6ae54db98.png" alt="Inheritance diagram of SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation" usemap="#inheritance61be618588" class="inheritance graphviz" /></div>
<map id="inheritance61be618588" name="inheritance61be618588">
<area shape="rect" id="node2" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation" target="_top" title="PromptEvaluation class." alt="" coords="157,5,292,31"/>
</map><dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/prompt_evaluation.html#PromptEvaluation.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation.__init__" title="Link to this definition"></a></dt>
<dd><p>Constructor.</p>
<p>Initializes the PromptEvaluation class.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation._calculate_prompt_metrics_for_language">
<span class="sig-name descname"><span class="pre">_calculate_prompt_metrics_for_language</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">language</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">LanguageResultsProcessor</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/prompt_evaluation.html#PromptEvaluation._calculate_prompt_metrics_for_language"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation._calculate_prompt_metrics_for_language" title="Link to this definition"></a></dt>
<dd><p>Calculates prompt evaluation metrics for a specific language.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>language</strong> (<em>str</em>) – Language for which metrics are calculated.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>LanguageResultsProcessor instance containing calculated metrics
and analysis tools.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>LanguageResultsProcessor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation._find_other_prompts">
<span class="sig-name descname"><span class="pre">_find_other_prompts</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">key</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/prompt_evaluation.html#PromptEvaluation._find_other_prompts"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation._find_other_prompts" title="Link to this definition"></a></dt>
<dd><p>Finds prompt ingredient sets that contain the given key/value pair.</p>
<p>Finds entries in the prompt ingredient sets that contain the specified
value for the specified key.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>key</strong> (<em>Any</em>) – The key to look up within the prompt ingredient sets.</p></li>
<li><p><strong>value</strong> (<em>Any</em>) – The value that must match for the given key in the prompt
ingredient sets.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>list containing the numbers of all matched prompt ingredient sets.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>List[Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation._get_group_ingredients_sets">
<span class="sig-name descname"><span class="pre">_get_group_ingredients_sets</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prompt_group</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/prompt_evaluation.html#PromptEvaluation._get_group_ingredients_sets"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation._get_group_ingredients_sets" title="Link to this definition"></a></dt>
<dd><p>Gets the prompt ingredient sets for the specified prompt group.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>prompt_group</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) – List of prompt numbers.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Dictionary with prompt numbers as keys and prompt ingredient sets
as values.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dict[int, Dict[str, str]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation._mark_invalid_prompts">
<span class="sig-name descname"><span class="pre">_mark_invalid_prompts</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">others</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/prompt_evaluation.html#PromptEvaluation._mark_invalid_prompts"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation._mark_invalid_prompts" title="Link to this definition"></a></dt>
<dd><p>Surrounds invalid prompt numbers in parentheses.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>others</strong> (<em>List</em><em>[</em><em>str</em><em>]</em>) – List of prompt numbers.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>List of prompt numbers with invalid prompt numbers surrounded in
parentheses.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>List[str]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation._reset_language_dependent_properties">
<span class="sig-name descname"><span class="pre">_reset_language_dependent_properties</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/prompt_evaluation.html#PromptEvaluation._reset_language_dependent_properties"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation._reset_language_dependent_properties" title="Link to this definition"></a></dt>
<dd><p>Resets the language-dependent properties.</p>
<p>This method reinitializes the ‘language_results_processor’ property of
the class by assigning it to None so that next time the processor is
needed, it must be reinitialized with the updated language setting.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation.all_languages_evaluation">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">all_languages_evaluation</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation" title="SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation"><span class="pre">AllLanguagesEvaluation</span></a></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation.all_languages_evaluation" title="Link to this definition"></a></dt>
<dd><p>Returns an instance of the AllLanguagesEvaluation class.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>An instance of the AllLanguagesEvaluation class which provides
various methods for the evaluation of the sentiment analysis
results for all languages that have been processed.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation" title="SentimentAnalysis.src.sentiment_analysis.evaluation.all_languages_evaluation.AllLanguagesEvaluation">AllLanguagesEvaluation</a></p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the AllLanguagesEvaluation instance has not been set.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation.evaluate_prompt_group">
<span class="sig-name descname"><span class="pre">evaluate_prompt_group</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prompt_group</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/prompt_evaluation.html#PromptEvaluation.evaluate_prompt_group"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation.evaluate_prompt_group" title="Link to this definition"></a></dt>
<dd><p>Evaluates the specified group of prompts.</p>
<p>Analyzes the provided group of prompts by inspecting the
corresponding ingredient sets and searching for hints why the group
performed in the way it did. It establishes which ingredients were
used throughout the group and which ingredients varied. It
especially identifies the ingredients that were not used in any
other prompts.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>prompt_group</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) – A list of integer identifiers representing the prompt numbers of
the members of the group to be evaluated.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The results of the evaluation are displayed in the console.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation.evaluate_prompts">
<span class="sig-name descname"><span class="pre">evaluate_prompts</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">partial_metrics</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/prompt_evaluation.html#PromptEvaluation.evaluate_prompts"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation.evaluate_prompts" title="Link to this definition"></a></dt>
<dd><p>Evaluates the prompts for all languages.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>partial_metrics</strong> (<em>List</em><em>[</em><em>str</em><em>] </em><em>| </em><em>None</em>) – A list of specific metrics to display for the best and worst</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation.evaluate_prompts_for_language">
<span class="sig-name descname"><span class="pre">evaluate_prompts_for_language</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">language</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">partial_metrics</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">LanguageResultsProcessor</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/prompt_evaluation.html#PromptEvaluation.evaluate_prompts_for_language"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation.evaluate_prompts_for_language" title="Link to this definition"></a></dt>
<dd><p>Evaluates the prompts for the given language.</p>
<p>Calculates the metrics and displays correlation maps and partial
metrics for the number of best and worst prompts specified in the
sentiment analysis configuration and finally for all prompts.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>language</strong> (<em>str</em>) – Language code of the language for which the prompts are to be
evaluated.</p></li>
<li><p><strong>partial_metrics</strong> (<em>List</em><em>[</em><em>str</em><em>]</em>) – List of partial metrics to be analyzed. The partial metrics names
can be chosen from the following list:
- ‘macro’,
- ‘f1’,
- ‘precision’,
- ‘recall’,
- ‘positive’,
- ‘negative’,
- ‘neutral’
E.g., ‘macro’ will calculate and display all metrics belonging
to the macro metrics, i.e. macro f1 score, macro precision,
macro recall and the accuracy, ‘positive’ will calculate and
display all metrics concerning the samples being classified as
‘positive’ by the sentiment analysis.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>An instance of the LanguageResultsProcessor, enabling the caller
to perform further statistical analyses, if required.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>LanguageResultsProcessor</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>ATTENTION: Before you run this method you should ensure you have moved
the generated chunks files in the csv data folder to a subfolder named
“<a href="#id9"><span class="problematic" id="id10">chunks_v_</span></a>” + the two-digit number of the prompt engineering strategy
it was created by (e.g. “chunks_v_01”).</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation.language">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">language</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation.language" title="Link to this definition"></a></dt>
<dd><p>Retrieves the language from the configuration settings.</p>
<p>If the language is not set, an exception is raised to
indicate the critical nature of the missing configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The language retrieved from the configuration.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>str</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="SentimentAnalysis.src.sentiment_analysis.retrieval.html#SentimentAnalysis.src.sentiment_analysis.retrieval.custom_exceptions.CriticalException" title="SentimentAnalysis.src.sentiment_analysis.retrieval.custom_exceptions.CriticalException"><strong>CriticalException</strong></a> – If the language is not set in the configuration.</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation.language_results_processor">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">language_results_processor</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">LanguageResultsProcessor</span></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation.language_results_processor" title="Link to this definition"></a></dt>
<dd><p>Returns an instance of the LanguageResultsProcessor class.</p>
<p>The language results processor provides functionality required
for processing the results of a language analysis. If the
processor does not already exist, this method initializes it by
invoking the ‘_set_language_results_processor’ method and then
returns it.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The language results processor for handling language-related
results.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>LanguageResultsProcessor</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation.languages">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">languages</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.prompt_evaluation.PromptEvaluation.languages" title="Link to this definition"></a></dt>
<dd><p>Returns the list of languages that are to be evaluated.</p>
<p>If the languages have not been initialized, it retrieves them from
the llm set in the sentiment analysis configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A list of language codes, corresponding to languages to evaluate.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>List[str]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation">
<span id="sentimentanalysis-src-sentiment-analysis-evaluation-single-prompt-evaluation-module"></span><h2>SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation module<a class="headerlink" href="#module-SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation" title="Link to this heading"></a></h2>
<section id="single-prompt-evaluation-py">
<h3>single_prompt_evaluation.py<a class="headerlink" href="#single-prompt-evaluation-py" title="Link to this heading"></a></h3>
<p>Version 1.0, updated on 2024-12-15</p>
</section>
<dl class="py class">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation.SinglePromptEvaluation">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation.</span></span><span class="sig-name descname"><span class="pre">SinglePromptEvaluation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">correct_labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="SentimentAnalysis.src.stats.html#SentimentAnalysis.src.stats.labels.Labels" title="SentimentAnalysis.src.stats.labels.Labels"><span class="pre">Labels</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">col</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Series</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">language</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'en'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/single_prompt_evaluation.html#SinglePromptEvaluation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation.SinglePromptEvaluation" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">LoggingMixin</span></code></p>
<p>SinglePromptEvaluation class</p>
<div class="graphviz"><img src="_images/inheritance-483c8a983329171182923b9b2e3ac6fbd1678f5d.png" alt="Inheritance diagram of SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation.SinglePromptEvaluation" usemap="#inheritanceb41d92e7ea" class="inheritance graphviz" /></div>
<map id="inheritanceb41d92e7ea" name="inheritanceb41d92e7ea">
<area shape="rect" id="node2" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation.SinglePromptEvaluation" target="_top" title="SinglePromptEvaluation class" alt="" coords="157,5,332,31"/>
</map><dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation.SinglePromptEvaluation.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">correct_labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="SentimentAnalysis.src.stats.html#SentimentAnalysis.src.stats.labels.Labels" title="SentimentAnalysis.src.stats.labels.Labels"><span class="pre">Labels</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">col</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Series</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">language</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'en'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/single_prompt_evaluation.html#SinglePromptEvaluation.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation.SinglePromptEvaluation.__init__" title="Link to this definition"></a></dt>
<dd><p>Initializes the SinglePromptEvaluation class with the given parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>correct_labels</strong> (<a class="reference internal" href="SentimentAnalysis.src.stats.html#SentimentAnalysis.src.stats.labels.Labels" title="SentimentAnalysis.src.stats.labels.Labels"><em>Labels</em></a>) – The correct labels the predicted labels are to be compared to.</p></li>
<li><p><strong>col</strong> (<em>Series</em>) – The column containing the predicted labels.</p></li>
<li><p><strong>language</strong> (<em>str</em>) – The language for which the prompt is used.</p></li>
<li><p><strong>col_name</strong> (<em>str</em>) – The name of the column with the classification results to evaluate.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation.SinglePromptEvaluation.compare_freqs">
<span class="sig-name descname"><span class="pre">compare_freqs</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/single_prompt_evaluation.html#SinglePromptEvaluation.compare_freqs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation.SinglePromptEvaluation.compare_freqs" title="Link to this definition"></a></dt>
<dd><p>Compares the frequencies of correct labels and predicted labels.</p>
<p>Outputs the comparison result to the console.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation.SinglePromptEvaluation.compute_metrics">
<span class="sig-name descname"><span class="pre">compute_metrics</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="reference internal" href="_modules/SentimentAnalysis/src/sentiment_analysis/evaluation/single_prompt_evaluation.html#SinglePromptEvaluation.compute_metrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation.SinglePromptEvaluation.compute_metrics" title="Link to this definition"></a></dt>
<dd><p>Computes the metrics for the answer column passed to this class.</p>
<p>Computes the metrics of the predicted labels based on the correct
labels.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation.SinglePromptEvaluation.correct_labels">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">correct_labels</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="SentimentAnalysis.src.stats.html#SentimentAnalysis.src.stats.labels.Labels" title="SentimentAnalysis.src.stats.labels.Labels"><span class="pre">Labels</span></a></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation.SinglePromptEvaluation.correct_labels" title="Link to this definition"></a></dt>
<dd><p>Gets the correct labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="SentimentAnalysis.src.sentiment_analysis.retrieval.html#SentimentAnalysis.src.sentiment_analysis.retrieval.custom_exceptions.CriticalException" title="SentimentAnalysis.src.sentiment_analysis.retrieval.custom_exceptions.CriticalException"><strong>CriticalException</strong></a> – If no correct labels are set.</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation.SinglePromptEvaluation.predicted_freqs">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">predicted_freqs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation.SinglePromptEvaluation.predicted_freqs" title="Link to this definition"></a></dt>
<dd><p>Returns the prompt name and the frequencies of the predicted labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A tuple, where the first element is a string representing a
prompt’s name und the second element a list of tuples where the
first element is a sentiment label and the second element the
frequency of the first element.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>Tuple[str, List[Tuple[str, int]]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation.SinglePromptEvaluation.predicted_labels">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">predicted_labels</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Series</span></em><a class="headerlink" href="#SentimentAnalysis.src.sentiment_analysis.evaluation.single_prompt_evaluation.SinglePromptEvaluation.predicted_labels" title="Link to this definition"></a></dt>
<dd><p>Gets the predicted labels.</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-SentimentAnalysis.src.sentiment_analysis.evaluation">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-SentimentAnalysis.src.sentiment_analysis.evaluation" title="Link to this heading"></a></h2>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="SentimentAnalysis.src.sentiment_analysis.html" class="btn btn-neutral float-left" title="SentimentAnalysis.src.sentiment_analysis package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="SentimentAnalysis.src.sentiment_analysis.prompt_engineering.html" class="btn btn-neutral float-right" title="SentimentAnalysis.src.sentiment_analysis.prompt_engineering package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Diane Keller.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>