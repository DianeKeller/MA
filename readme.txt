===================================
SentimentAnalysis
-----------------
Version 1.0, updated on 2025-05-07

===================================


Installation
============
1. Having cloned the SentimentAnalysis program, you need to run the 'setup.py'
module to install the required libraries for running the program: Switch to
the SentimentAnalysis folder, open a command line interpreter and call the
setup module like this:

>>> python -m setup

2. To query BLOOM, you need to create a Hugging Face account where you need
to generate a Hugging Face User Access Token. Store the token in a local
environment variable called 'HUGGING_FACE_AUTH_TOKEN' in your system. The
SentimentAnalysis program will use this to authenticate you when it tries to
retrieve sentiments from BLOOM.


Running the SentimentAnalysis program
=====================================
1.  To run the SentimentAnalysis program from the command line, switch to the
    SentimentAnalysis folder, open a command line interpreter and call the
    sa module like this:

    >>> python -m sa

    You will be shown the module's documentation which will explain how to use
    the module.

2.  Other entry points that might be useful to call are:

    >>> python -m src.data_sources.mad_tsc_workflow
        This will load the MAD-TSC datasets, if they need to be loaded separately
        from the rest of the program.

    >>> python -m src.utils.sys_utils
        This will print all system and python information made available in the
        sys_utils module.

3.  From an IDE like PyCharm, the main module may be used to run or debug the program.


Special Coding Conventions Followed in This Program
===================================================

Line Length:
------------
Max. 79 characters

Parameter lists are often formatted to present each parameter in a single
line, because this way, they can be easily added to or removed from the list,
and be assorted by type hints without reaching the line length limit. One
parameter per line makes it also easier to read and grasp which parameters
are used.

Method Names:
-------------
- "compute"  Indicates that the method does not return a value but stores the
             computed result in a property

- "set"      Is similar to "compute". It is often used to enable the use of
             decorators with a property since properties do not allow the
             application of decorators other than the "property" and "setter"
             decorators themselves. In this case, the setter method calls
             the decorated "set" method.

             Otherwise, it is used instead of "compute" if the assignment of
             the value to set is straightforward or the detailed computation
             of the value is performed by another method that is called from
             the "set" method.

- "get"      Indicates that the method returns a value. Used if the method
             needs input parameters or if a property needs to check values
             using decorators.

- "use"      Indicates that the method guarantees that the value returned is
             not None.


Variable and Method Names:
--------------------------
- "n_"           = "Number of"

- "val"          = "Value"

- "col"/"cols"   = "Column"/"columns"

- "freq"/"freqs" = "Frequency"/"frequencies"

- "dir"          = "Directory"

- "dict"         = "Dictionary"

- "lang"         = "Language"

- "df"           Often used as a variable name for a DataFrame object

- "my_df"        Often used as a variable name for a MyDataFrame object

- "attr"        = "Attribute"


Comments:
---------
- "LLM"         = "Large language model"


Terminology:
------------
In this program, a 'prompt' refers to a dictionary containing prompt
components used to construct specific 'queries' for given samples. In
contrast, a 'query' is the text generated by combining a prompt's
components with a sample's text and relevant details, forming the
payload sent to an API.

Prompt
    Dictionary containing 6 positional categories:
    - before_sentence
    - before_mention
    - scale
    - question
    - answer_before_mention
    - answer_start
    This dictionary is used to compose the concrete query strings for each
    sample sentence inserting the data columns of each sample at its
    corresponding position. The columns are the following:

    - SENTENCE_NORMALISED
    - MENTION

Query
    Each query string is composed as follows from the prompt dictionary and
    the sample columns:

    <before_sentence>[SENTENCE_NORMALISED]<before_mention>[MENTION]<scale>
    <question><answer_before_mention>[MENTION]<answer_start>

Prompt ingredients
    Dictionary containing lists of equivalent values for prompt ingredients
    categories like "question" or "scale". Beginning with the second prompt
    engineering strategy (PromptEngineeringStrategy2), a systematic
    generation of prompts from prompt ingredients lists is developed.The
    resulting dictionaries contain 13 categories:
    - question
    - answer_start
    - answer_before_mention
    - scale
    - politeness
    - what
    - preposition
    - given
    - where
    - toward
    - target
    - sentence_label
    - task

    This dictionary is used to compose the 6 positional categories of the
    prompts.

Inherited parameters
    Parameters required during the instantiation of a class, that do not
    appear explicitly in the class definition because the class does not
    define its own constructor (__init__ method). Instead, the constructor is
    inherited from the parent class, which handles the initialization process.
    As a result, the parameters are passed to the parent class's
    __init__ method when creating an instance of the subclass.

Validated
    A module is called "validated" if at least all the 'happy paths' in the
    module have been executed at least once (by running the test methods or
    by normally excecuting the project's code). Exceptions might not have
    been caused to be raised.



Some Special Programming Techniques
===================================
The program was designed not only to read in and use a single data collection
and interact with and query a single LLM, but also to allow for the easy
addition of other datasets and LLMs for comparison purposes. The program's
structure was therefore created to facilitate relatively simple extension.

Strategy Pattern
----------------
To ensure expandability with alternative objects and processing routines, the
program frequently employs the Strategy Pattern. This design pattern allows
program behavior to switch at runtime based on the given context. A typical
example in the program is the implementation of serialization functions. This
involves a Serializer class and various serialization strategies, such as a
CSV strategy or a JSON strategy. To serialize or deserialize a data structure
(e.g., a pandas DataFrame or a dictionary), the Serializer is informed about
which serialization strategy to use in a specific case for typical methods
like save, load, add, and delete. The Serializer then delegates serialization
tasks to the methods of the specified serialization strategy, which contain
the actual code for the (de)serialization of the data structure in the
required format and location.

The advantage of this behavior pattern lies in the encapsulation of the code
for different strategies, decoupling the context (in this case, the
Serializer) from the strategy classes, adherence to the Open/Closed Principle,
improved clarity and maintainability, seamless expandability, and uniform and
straightforward invocation of serialization functions from the rest of the
program. This eliminates the need for the rest of the program to handle the
correct methods and parameters for serialization in the correct directory. For
the program, serialization strategies were implemented for CSV, JSON, JSONL,
PKL, and TXT files. If needed, these can easily be extended to support other
file formats such as ARROW, XLS, or XLSX.

The strategy pattern is also used in the program for authenticating the user
for access to data sources and language models, processing data sources,
tokenization and creating statistics.

Mixins
------
Unlike older programming languages like Java and C#, Python offers the ability
to use Mixins. Mixins allow the addition of properties and methods to existing
classes without establishing a traditional inheritance relationship. They are
used for composing reusable modules and promoting the Separation of Concerns
by bundling related properties and methods, extracting them from classes, and
incorporating them into different classes as needed.

In this project, Mixins are used in several places. For instance,
-   the FactSheetMixins for the MAD-TSC corpus and BLOOM hold information
    about these resources and provide formatted descriptions.
-   The DataSourceStatsMixin offers statistical methods for data source
    suites.
-   The TokenizationMixin provides tokenization functions that can be
    incorporated into various classes.
-   The MetricsVisualizationMixin provides visualization functions for
    evaluating metrics collected during sentiment analysis.
-   The ServerlessBloomPromptValidationMixin adds properties and methods for
    prompt validation.
-   The DataSerializationMixin simplifies data serialization in various parts
    of the program.
-   The most frequently used LoggingMixin integrates logging functions into
    most of the program's classes.
