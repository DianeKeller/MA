"""
prompt_evaluation.py
--------------------
Version 1.0, updated on 2025-02-05

"""

from typing import List, Dict, Any

from logger import Logger
from src.logging_mixin import LoggingMixin
from src.sentiment_analysis.evaluation.all_languages_evaluation import (
    AllLanguagesEvaluation
)
from src.sentiment_analysis.evaluation.language_results_processor import (
    LanguageResultsProcessor
)
from src.sentiment_analysis.prompt_engineering.prompt_engineer_factory \
    import get_prompt_engineer
from src.sentiment_analysis.retrieval.custom_exceptions import (
    CriticalException
)
from src.sentiment_analysis.sentiment_analysis_config import (
    SentimentAnalysisConfig
)
from src.utils.data_utils import is_none_or_empty
from src.utils.dict_utils import (
    get_inner_keys_of_dict_of_dict,
    get_unique_values_for_inner_key
)
from src.utils.list_utils import (
    find_list_of_dicts_entries_with_same_values_for_inner_key,
    remove_elements_from_list
)
from src.utils.print_utils import print_in_box, WHITE_SPACE
from type_aliases import PromptsDictType


class PromptEvaluation(LoggingMixin):
    """
    PromptEvaluation class.

    This class initializes and manages objects required for the evaluation of
    sentiment analysis results. It provides logging capabilities through the
    'LoggingMixin' class.

    Attributes
    ----------
    config : SentimentAnalysisConfig
        The sentiment analysis configuration object that holds configuration
        values and settings for the analysis process.

    prompt_engineer : PromptEngineer
        The prompt engineer instance responsible for generating prompt
        configurations based on the provided version.

    prompt_ingredients_sets: PromptsDictType
        The collection of prompt ingredient sets generated by the prompt
        engineer.

    
    """

    def __init__(self) \
            -> None:
        """
        Constructor.

        Initializes the PromptEvaluation class.


        """

        self._language_results_processor: (
                LanguageResultsProcessor | None
        ) = None

        self._languages = None
        self._all_languages_evaluation: AllLanguagesEvaluation | None = None

        # Override the default logger of the 'LoggingMixin' class.
        self.logger = Logger(self.__class__.__name__).get_logger()

        # Make the sentiment analysis configuration available in this class
        self.config = SentimentAnalysisConfig()

        # Make the prompt ingredients sets available in this class
        self.prompt_engineer = get_prompt_engineer(int(self.config.get(
            'version')))

        self.prompt_ingredients_sets: PromptsDictType = (
            self.prompt_engineer.get_prompt_ingredients_sets())

    # region --- Properties
    @property
    def languages(self) \
            -> List[str]:
        """
        Returns the list of languages that are to be evaluated.

        If the languages have not been initialized, it retrieves them from
        the llm set in the sentiment analysis configuration.

        Returns
        -------
        List[str]
            A list of language codes, corresponding to languages to evaluate.

        """

        if is_none_or_empty(self._languages):
            # Get the languages that are to be evaluated
            self.languages = self.config.get('llm').compatible_languages

        return self._languages

    @languages.setter
    def languages(self, languages: List[str]) \
            -> None:
        """
        Sets the list of languages to evaluate.

        Parameters
        ----------
        languages : List[str]
            A list of language codes representing languages to evaluate.

        """

        self._languages = languages

    @property
    def language(self) \
            -> str:
        """
        Retrieves the language from the configuration settings.

        If the language is not set, an exception is raised to
        indicate the critical nature of the missing configuration.

        Returns
        -------
        str
            The language retrieved from the configuration.

        Raises
        ------
        CriticalException
            If the language is not set in the configuration.

        """

        if not self.config.get('language'):
            raise CriticalException(
                self.logger,
                "Language is not set!"
            )

        return self.config.get('language')

    @language.setter
    def language(self, language: str) \
            -> None:
        """
        Sets the language and resets properties dependent on the language.

        This setter uses the specified language to set the corresponding
        setting in the sentiment analysis configuration. It performs a reset
        operation on any properties that depend on the selected
        language. Typically used to update the language dynamically within the
        application while ensuring coherence in dependent components.

        Parameters
        ----------
        language : str
            The new language to set in the configuration.

        """

        self.config.set('language', language)
        self._reset_language_dependent_properties()

    @property
    def all_languages_evaluation(self) \
            -> AllLanguagesEvaluation:
        """
        Returns an instance of the AllLanguagesEvaluation class.

        Returns
        -------
        AllLanguagesEvaluation
            An instance of the AllLanguagesEvaluation class which provides
            various methods for the evaluation of the sentiment analysis
            results for all languages that have been processed.

        Raises
        ------
        NotImplementedError
            if the AllLanguagesEvaluation instance has not been set.

        """

        if not self._all_languages_evaluation:
            msg = ("AllLanguagesEvaluation has not been initialized yet. "
                   "Please initialize it before attempting to access this "
                   "property!")
            self._log(msg, 'error')
            raise NotImplementedError

        return self._all_languages_evaluation

    @all_languages_evaluation.setter
    def all_languages_evaluation(
            self,
            all_languages_evaluation: AllLanguagesEvaluation
    ) -> None:
        """
        Sets an instance of the AllLanguagesEvaluation class.

        Parameters
        ----------
        all_languages_evaluation : AllLanguagesEvaluation
            The instance of the AllLanguagesEvaluation class to set the
            property with.

        """

        self._all_languages_evaluation = all_languages_evaluation

    @property
    def language_results_processor(self) \
            -> LanguageResultsProcessor:
        """
        Returns an instance of the LanguageResultsProcessor class.

        The language results processor provides functionality required
        for processing the results of a language analysis. If the
        processor does not already exist, this method initializes it by
        invoking the '_set_language_results_processor' method and then
        returns it.

        Returns
        -------
        LanguageResultsProcessor
            The language results processor for handling language-related
            results.

        """

        if self._language_results_processor is None:
            self._set_language_results_processor()

        return self._language_results_processor

    @language_results_processor.setter
    def language_results_processor(self, processor: LanguageResultsProcessor) \
            -> None:
        """
        Sets the language results processor with the provided instance.

        Sets the language results processor used for processing
        language-specific results. This function allows associating a
        LanguageResultsProcessor object with the current class instance.

        Parameters
        ----------
        processor : LanguageResultsProcessor
            An instance of LanguageResultsProcessor that performs processing
            of language-specific results.

        """

        self._language_results_processor = processor

    # endregion --- Properties

    # region --- Public Methods

    def evaluate_prompts(
            self,
            partial_metrics: List[str] | None = None
    ) -> None:
        """
        Evaluates the prompts for all languages.

        Parameters
        ----------
        partial_metrics : List[str] | None
            A list of specific metrics to display for the best and worst

        """

        self.all_languages_evaluation = AllLanguagesEvaluation()
        all_languages_evaluation = self.all_languages_evaluation

        for language in self._languages:
            language_results_processor = self.evaluate_prompts_for_language(
                language,
                partial_metrics
            )
            all_languages_evaluation.add_to_overall_metrics(
                language_results_processor.metrics)
            all_languages_evaluation.add_to_overall_sentiment_data(
                language_results_processor.data
            )

        # Remove language setting
        self.config.remove('language')

        all_languages_evaluation.show_overall_rankings()

        all_languages_evaluation.show_mean_metrics()

        all_languages_evaluation.show_freqs_comparisons_by_prompt()
        all_languages_evaluation._show_pairwise_freqs_comparisons()

    def evaluate_prompts_for_language(
            self,
            language: str,
            partial_metrics: List[str] | None = None
    ) -> LanguageResultsProcessor:
        """
        Evaluates the prompts for the given language.

        Calculates the metrics and displays correlation maps and partial
        metrics for the number of best and worst prompts specified in the
        sentiment analysis configuration and finally for all prompts.

        Parameters
        ----------
        language: str
            Language code of the language for which the prompts are to be
            evaluated.

        partial_metrics: List[str]
            List of partial metrics to be analyzed. The partial metrics names
            can be chosen from the following list:
            - 'macro',
            - 'f1',
            - 'precision',
            - 'recall',
            - 'positive',
            - 'negative',
            - 'neutral'
            E.g., 'macro' will calculate and display all metrics belonging
            to the macro metrics, i.e. macro f1 score, macro precision,
            macro recall and the accuracy, 'positive' will calculate and
            display all metrics concerning the samples being classified as
            'positive' by the sentiment analysis.


        Returns
        -------
        LanguageResultsProcessor
            An instance of the LanguageResultsProcessor, enabling the caller
            to perform further statistical analyses, if required.

        Notes
        -----
        ATTENTION: Before you run this method you should ensure you have moved
        the generated chunks files in the csv data folder to a subfolder named
        "chunks_v_" + the two-digit number of the prompt engineering strategy
        it was created by (e.g. "chunks_v_01").

        """

        self.config.set('language', language)

        processor = self.language_results_processor

        processor.analyze_prompts(partial_metrics)

        if not is_none_or_empty(partial_metrics):
            processor.show_partial_metrics(partial_metrics)

        return processor

    def evaluate_prompt_group(self, prompt_group: List[int]) \
            -> None:
        """
        Evaluates the specified group of prompts.

        Analyzes the provided group of prompts by inspecting the
        corresponding ingredient sets and searching for hints why the group
        performed in the way it did. It establishes which ingredients were
        used throughout the group and which ingredients varied. It
        especially identifies the ingredients that were not used in any
        other prompts.

        Parameters
        ----------
        prompt_group : List[int]
            A list of integer identifiers representing the prompt numbers of
            the members of the group to be evaluated.

        Notes
        -----
        The results of the evaluation are displayed in the console.

        """

        group_ingredients_sets = self._get_group_ingredients_sets(prompt_group)
        keys = get_inner_keys_of_dict_of_dict(group_ingredients_sets)

        # Initialize lists to store the results
        only_in_group: List[str] = []
        variations: List[str] = []
        inconspicious: List[str] = []
        extended_inconspicious: List[str] = []

        for key in sorted(keys):
            values = get_unique_values_for_inner_key(group_ingredients_sets, key)

            if len(values) == 1:
                # Find other prompts that use the same ingredients and
                # exclude the members of the group from the other prompts
                # having used the same ingredients
                others = remove_elements_from_list(
                    self._find_other_prompts(key, values[0]),
                    prompt_group
                )

                if len(others) == 0:
                    only_in_group.append(f'{str(key)}:  {str(values[0])}')

                else:
                    inconspicious.append(
                        f'{str(key)}:  {str(values[0])} '
                    )

                    # Mark invalid others as invalid
                    revised_others = self._mark_invalid_prompts(others)

                    extended_inconspicious.append(
                        f'{str(key)}:  {str(values[0])} \n'
                        f'Others: {str(revised_others)} \n{WHITE_SPACE}'
                    )
            else:
                variations.append(f'{str(key)}:  {str(values)}')

        title = f"Only in group {prompt_group}"
        body = only_in_group

        print_in_box(title, body)

        title = "Variations"
        body = variations

        print_in_box(title, body)

        title = "Inconspicious"
        body = inconspicious

        print_in_box(title, body)

        title = "Inconspicious with invalid prompts marked by brackets"
        body = extended_inconspicious

        print_in_box(title, body)

    # endregion --- Public Methods

    # region --- Protected Methods

    def _find_other_prompts(self, key: Any, value: Any) \
            -> List[Any]:
        """
        Finds prompt ingredient sets that contain the given key/value pair.

        Finds entries in the prompt ingredient sets that contain the specified
        value for the specified key.

        Parameters
        ----------
        key : Any
            The key to look up within the prompt ingredient sets.

        value : Any
            The value that must match for the given key in the prompt
            ingredient sets.

        Returns
        -------
        List[Any]
            list containing the numbers of all matched prompt ingredient sets.

        """

        return find_list_of_dicts_entries_with_same_values_for_inner_key(
            self.prompt_ingredients_sets.data, key, value
        )

    def _calculate_prompt_metrics_for_language(
            self,
            language: str
    ) -> LanguageResultsProcessor:
        """
        Calculates prompt evaluation metrics for a specific language.

        Parameters
        ----------
        language : str
            Language for which metrics are calculated.

        Returns
        -------
        LanguageResultsProcessor
            LanguageResultsProcessor instance containing calculated metrics
            and analysis tools.

        """

        processor = LanguageResultsProcessor(language)
        processor.process_language()
        return processor

    def _get_group_ingredients_sets(self, prompt_group: List[int]) \
            -> Dict[int, Dict[str, str]]:
        """
        Gets the prompt ingredient sets for the specified prompt group.

        Parameters
        ----------
        prompt_group : List[int]
            List of prompt numbers.

        Returns
        -------
        Dict[int, Dict[str, str]]
            Dictionary with prompt numbers as keys and prompt ingredient sets
            as values.

        """

        group_ingredients_sets: Dict[int, Dict[str, str]] = {}

        for nr in prompt_group:
            group_ingredients_sets[nr] = (
                self.prompt_ingredients_sets.get_entry(nr))

        return group_ingredients_sets

    def _mark_invalid_prompts(self, others: List[str]) \
            -> List[str]:
        """
        Surrounds invalid prompt numbers in parentheses.

        Parameters
        ----------
        others : List[str]
            List of prompt numbers.

        Returns
        -------
        List[str]
            List of prompt numbers with invalid prompt numbers surrounded in
            parentheses.

        """

        revised_others = []

        for other in others:

            processor = self.language_results_processor

            answer_cols = processor.data.answer_cols

            if ('answer_' + str(other) not in
                    answer_cols):
                revised_others.append('(' + str(other) + ')')
            else:
                revised_others.append(str(other))

        return revised_others

    def _reset_language_dependent_properties(self) \
            -> None:
        """
        Resets the language-dependent properties.

        This method reinitializes the 'language_results_processor' property of
        the class by assigning it to None so that next time the processor is
        needed, it must be reinitialized with the updated language setting.

        """

        self.language_results_processor = None

    def _set_language_results_processor(self) \
            -> None:

        self._language_results_processor = (
            self._calculate_prompt_metrics_for_language(
                self.config.get('language')
            )
        )

    # endregion --- Protected Methods
